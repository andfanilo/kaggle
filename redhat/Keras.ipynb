{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse as ssp\n",
    "import pylab as plt\n",
    "from sklearn.preprocessing import LabelEncoder,LabelBinarizer,MinMaxScaler,OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD,NMF,PCA,FactorAnalysis\n",
    "from sklearn.feature_selection import SelectFromModel,SelectPercentile,f_classif\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import log_loss,roc_auc_score\n",
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "from sklearn.cross_validation import StratifiedKFold,KFold\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import ModelCheckpoint,Callback\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Embedding, LSTM, Dense,Flatten, Dropout, merge,Convolution1D,MaxPooling1D,Lambda,AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.advanced_activations import PReLU,LeakyReLU,ELU,SReLU\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "path = \"data/\"\n",
    "dim = 32\n",
    "hidden = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A callback is a set of functions to be applied at given stages of the training procedure. \n",
    "#You can use callbacks to get a view on internal states and statistics of the model during training. \n",
    "#You can pass a list of callbacks (as the keyword argument callbacks) to the .fit() method of the Sequential model. \n",
    "#The relevant methods of the callbacks will then be called at each stage of the training.\n",
    "\n",
    "class AucCallback(Callback):  #inherits from Callback\n",
    "    \n",
    "    def __init__(self, validation_data=(), patience=25,is_regression=True,best_model_name='best_keras.mdl',feval='roc_auc_score',batch_size=1024*8):\n",
    "        super(Callback, self).__init__()\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.X_val, self.y_val = validation_data  #tuple of validation X and y\n",
    "        self.best = -np.inf\n",
    "        self.wait = 0  #counter for patience\n",
    "        self.best_model=None\n",
    "        self.best_model_name = best_model_name\n",
    "        self.is_regression = is_regression\n",
    "        self.y_val = self.y_val#.astype(np.int)\n",
    "        self.feval = feval\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        p = self.model.predict(self.X_val,batch_size=self.batch_size, verbose=0)#.ravel()\n",
    "        if self.feval=='roc_auc_score':\n",
    "            current = roc_auc_score(self.y_val,p)\n",
    "\n",
    "        if current > self.best:\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            self.model.save_weights(self.best_model_name,overwrite=True)\n",
    "            \n",
    "        else:\n",
    "            if self.wait >= self.patience:\n",
    "                self.model.stop_training = True\n",
    "                print('Epoch %05d: early stopping' % (epoch))\n",
    "                \n",
    "            self.wait += 1 #incremental the number of times without improvement\n",
    "        print('Epoch %d Auc: %f | Best Auc: %f \\n' % (epoch,current,self.best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_batches(size, batch_size):\n",
    "    nb_batch = int(np.ceil(size/float(batch_size)))\n",
    "    return [(i*batch_size, min(size, (i+1)*batch_size)) for i in range(0, nb_batch)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "train = pd.read_csv(path+'act_train.csv')\n",
    "test = pd.read_csv(path+'act_test.csv')\n",
    "people = pd.read_csv(path+'people.csv')\n",
    "test['outcome'] = np.nan\n",
    "data = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join people to activities\n",
    "data = pd.merge(data,people,how='left',on='people_id').fillna('missing')\n",
    "train = data[:train.shape[0]]\n",
    "test = data[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encode categorical values\n",
    "columns = train.columns.tolist()\n",
    "columns.remove('activity_id')\n",
    "columns.remove('people_id')\n",
    "columns.remove('outcome')\n",
    "data = pd.concat([train,test])\n",
    "for c in columns:\n",
    "    data[c] = LabelEncoder().fit_transform(data[c].values)\n",
    "\n",
    "train = data[:train.shape[0]]\n",
    "test = data[train.shape[0]:]\n",
    "\n",
    "data = pd.concat([train,test])\n",
    "columns = train.columns.tolist()\n",
    "columns.remove('activity_id')\n",
    "columns.remove('people_id')\n",
    "columns.remove('outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train[columns].values\n",
    "X_t = test[columns].values\n",
    "y = train[\"outcome\"].values\n",
    "people_id = train[\"people_id\"].values\n",
    "activity_id = test['activity_id']\n",
    "\n",
    "#del data\n",
    "#del train\n",
    "#del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(y, n_folds=4, shuffle=True, random_state=seed)\n",
    "for ind_tr, ind_te in skf:\n",
    "    X_train = X[ind_tr]\n",
    "    X_test = X[ind_te]\n",
    "\n",
    "    y_train = y[ind_tr]\n",
    "    y_test = y[ind_te]\n",
    "    break\n",
    "\n",
    "X_train = [X_train[:,i] for i in range(X.shape[1])]\n",
    "X_test = [X_test[:,i] for i in range(X.shape[1])]\n",
    "\n",
    "#del X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill in inputs layer\n",
    "flatten_layers = []\n",
    "inputs = []\n",
    "\n",
    "for c in columns:\n",
    "    inputs_c = Input(shape=(1,), dtype='int32', name=\"Input_\"+c)\n",
    "    num_c = len(np.unique(data[c].values))\n",
    "    embed_c = Embedding(\n",
    "                    num_c,\n",
    "                    dim,\n",
    "                    dropout=0.2,\n",
    "                    input_length=1,\n",
    "                    name=\"Embedding_\"+c\n",
    "                    )(inputs_c)\n",
    "    flatten_c= Flatten(name=\"Flatten_\"+c)(embed_c)\n",
    "    inputs.append(inputs_c)\n",
    "    flatten_layers.append(flatten_c)\n",
    "    \n",
    "flatten = merge(flatten_layers, mode='concat', name=\"Merge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Close model\n",
    "fc1 = Dense(hidden, activation='relu', name = 'Dense1')(flatten)\n",
    "dp1 = Dropout(0.5, name='Dropout1')(fc1)\n",
    "\n",
    "fc2 = Dense(hidden/2, activation='relu', name = 'Dense2')(dp1)\n",
    "dp2 = Dropout(0.5, name='Dropout2')(fc2)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(dp2)\n",
    "\n",
    "model = Model(input=inputs, output=outputs)\n",
    "model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = 'mlp_residual_%s_%s.hdf5'%(dim,hidden)\n",
    "model_checkpoint = ModelCheckpoint(model_name, monitor='val_loss', save_best_only=True)\n",
    "auc_callback = AucCallback(validation_data=(X_test,y_test), patience=5,is_regression=True,best_model_name=path+'best_keras.mdl',feval='roc_auc_score')\n",
    "\n",
    "nb_epoch = 10\n",
    "\n",
    "batch_size = 1024*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Model\n"
     ]
    }
   ],
   "source": [
    "print('Load Model')\n",
    "model.load_weights(path+model_name)\n",
    "# model.load_weights(path+'best_keras.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/datalab/lib/python3.4/site-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1647967 samples, validate on 549324 samples\n",
      "Epoch 1/10\n",
      "1647967/1647967 [==============================] - 105s - loss: 0.2630 - val_loss: 0.0800\n",
      "Epoch 2/10\n",
      "1647967/1647967 [==============================] - 98s - loss: 0.1284 - val_loss: 0.0679\n",
      "Epoch 3/10\n",
      "1647967/1647967 [==============================] - 92s - loss: 0.1210 - val_loss: 0.0644\n",
      "Epoch 4/10\n",
      "1647967/1647967 [==============================] - 115s - loss: 0.1153 - val_loss: 0.0620\n",
      "Epoch 5/10\n",
      "1647967/1647967 [==============================] - 127s - loss: 0.1138 - val_loss: 0.0593\n",
      "Epoch 6/10\n",
      "1647967/1647967 [==============================] - 106s - loss: 0.1115 - val_loss: 0.0558\n",
      "Epoch 7/10\n",
      "1647967/1647967 [==============================] - 107s - loss: 0.1060 - val_loss: 0.0510\n",
      "Epoch 8/10\n",
      "1647967/1647967 [==============================] - 104s - loss: 0.0991 - val_loss: 0.0456\n",
      "Epoch 9/10\n",
      "1647967/1647967 [==============================] - 112s - loss: 0.0969 - val_loss: 0.0417\n",
      "Epoch 10/10\n",
      "1647967/1647967 [==============================] - 111s - loss: 0.0937 - val_loss: 0.0405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f81729ebda0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    batch_size=batch_size, \n",
    "    nb_epoch=nb_epoch, \n",
    "    verbose=1, \n",
    "    shuffle=True,\n",
    "    validation_data=[X_test,y_test],\n",
    "    # callbacks = [\n",
    "        # model_checkpoint,\n",
    "        # auc_callback,\n",
    "        # ],\n",
    "    )\n",
    "\n",
    "# model.load_weights(model_name)\n",
    "# model.load_weights(path+'best_keras.mdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unknown format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2902035e9fdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/root/miniconda3/envs/datalab/lib/python3.4/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m    255\u001b[0m     return _average_binary_score(\n\u001b[1;32m    256\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/miniconda3/envs/datalab/lib/python3.4/site-packages/sklearn/metrics/base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unknown format is not supported"
     ]
    }
   ],
   "source": [
    "y_preds = model.predict(X_test,batch_size=1024*8)\n",
    "print(roc_auc_score(y_train, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('Make submission')\n",
    "X_t = [X_t[:,i] for i in range(X_t.shape[1])]\n",
    "outcome = model.predict(X_t,batch_size=1024*8)\n",
    "submission = pd.DataFrame()\n",
    "submission['activity_id'] = activity_id\n",
    "submission['outcome'] = outcome\n",
    "submission.to_csv('submission_residual_%s_%s.csv'%(dim,hidden),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###DO NOT DELETE, WORKING EXAMPLE\n",
    "inputs = [model.get_layer(\"Input_people_id\").input, K.learning_phase()]\n",
    "outputs = [model.get_layer(\"Flatten_people_id\").output]\n",
    "func = K.function(inputs, outputs)\n",
    "\n",
    "sample_size = data.shape[0]\n",
    "data_ae = []\n",
    "batches = make_batches(sample_size, batch_size)\n",
    "for batch_index, (batch_start, batch_end) in enumerate(batches):\n",
    "    X_0 = np.reshape(data[batch_start:batch_end][\"people_id\"], (-1, 1))\n",
    "    yy = func([X_0,0])[0]\n",
    "    data_ae.append(yy)\n",
    "\n",
    "\n",
    "data_ae = np.vstack(data_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = [model.get_layer(\"Input_people_id\").input, model.get_layer(\"Input_date_x\").input, K.learning_phase()]\n",
    "outputs = [model.get_layer(\"Flatten_people_id\").output, model.get_layer(\"Flatten_date_x\").output]\n",
    "func = K.function(inputs, outputs)\n",
    "\n",
    "sample_size = data.shape[0]\n",
    "data_ae = []\n",
    "batches = make_batches(sample_size, 1024)\n",
    "for batch_index, (batch_start, batch_end) in enumerate(batches):\n",
    "    X_0 = np.reshape(data[batch_start:batch_end][[\"people_id\"]], (-1, 1))\n",
    "    X_1 = np.reshape(data[batch_start:batch_end][[\"date_x\"]], (-1, 1))\n",
    "    yy = func([X_0,X_1,0])[0]\n",
    "    data_ae.append(yy)\n",
    "\n",
    "data_ae = np.vstack(data_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (0, 8192) 2695978 0.9969613995366431\n",
      "1 (8192, 16384) 2695978 0.9939227990732862\n",
      "2 (16384, 24576) 2695978 0.9908841986099293\n",
      "3 (24576, 32768) 2695978 0.9878455981465725\n",
      "4 (32768, 40960) 2695978 0.9848069976832156\n",
      "5 (40960, 49152) 2695978 0.9817683972198586\n",
      "6 (49152, 57344) 2695978 0.9787297967565017\n",
      "7 (57344, 65536) 2695978 0.9756911962931448\n",
      "8 (65536, 73728) 2695978 0.9726525958297879\n",
      "9 (73728, 81920) 2695978 0.969613995366431\n",
      "10 (81920, 90112) 2695978 0.9665753949030741\n",
      "11 (90112, 98304) 2695978 0.9635367944397173\n",
      "12 (98304, 106496) 2695978 0.9604981939763604\n",
      "13 (106496, 114688) 2695978 0.9574595935130035\n",
      "14 (114688, 122880) 2695978 0.9544209930496466\n",
      "15 (122880, 131072) 2695978 0.9513823925862896\n",
      "16 (131072, 139264) 2695978 0.9483437921229327\n",
      "17 (139264, 147456) 2695978 0.9453051916595758\n",
      "18 (147456, 155648) 2695978 0.9422665911962189\n",
      "19 (155648, 163840) 2695978 0.9392279907328621\n",
      "20 (163840, 172032) 2695978 0.9361893902695052\n",
      "21 (172032, 180224) 2695978 0.9331507898061483\n",
      "22 (180224, 188416) 2695978 0.9301121893427914\n",
      "23 (188416, 196608) 2695978 0.9270735888794345\n",
      "24 (196608, 204800) 2695978 0.9240349884160776\n",
      "25 (204800, 212992) 2695978 0.9209963879527207\n",
      "26 (212992, 221184) 2695978 0.9179577874893637\n",
      "27 (221184, 229376) 2695978 0.9149191870260069\n",
      "28 (229376, 237568) 2695978 0.91188058656265\n",
      "29 (237568, 245760) 2695978 0.9088419860992931\n",
      "30 (245760, 253952) 2695978 0.9058033856359362\n",
      "31 (253952, 262144) 2695978 0.9027647851725793\n",
      "32 (262144, 270336) 2695978 0.8997261847092224\n",
      "33 (270336, 278528) 2695978 0.8966875842458655\n",
      "34 (278528, 286720) 2695978 0.8936489837825086\n",
      "35 (286720, 294912) 2695978 0.8906103833191518\n",
      "36 (294912, 303104) 2695978 0.8875717828557949\n",
      "37 (303104, 311296) 2695978 0.8845331823924379\n",
      "38 (311296, 319488) 2695978 0.881494581929081\n",
      "39 (319488, 327680) 2695978 0.8784559814657241\n",
      "40 (327680, 335872) 2695978 0.8754173810023672\n",
      "41 (335872, 344064) 2695978 0.8723787805390103\n",
      "42 (344064, 352256) 2695978 0.8693401800756534\n",
      "43 (352256, 360448) 2695978 0.8663015796122966\n",
      "44 (360448, 368640) 2695978 0.8632629791489397\n",
      "45 (368640, 376832) 2695978 0.8602243786855828\n",
      "46 (376832, 385024) 2695978 0.8571857782222259\n",
      "47 (385024, 393216) 2695978 0.854147177758869\n",
      "48 (393216, 401408) 2695978 0.851108577295512\n",
      "49 (401408, 409600) 2695978 0.8480699768321551\n",
      "50 (409600, 417792) 2695978 0.8450313763687982\n",
      "51 (417792, 425984) 2695978 0.8419927759054414\n",
      "52 (425984, 434176) 2695978 0.8389541754420845\n",
      "53 (434176, 442368) 2695978 0.8359155749787276\n",
      "54 (442368, 450560) 2695978 0.8328769745153707\n",
      "55 (450560, 458752) 2695978 0.8298383740520138\n",
      "56 (458752, 466944) 2695978 0.8267997735886569\n",
      "57 (466944, 475136) 2695978 0.8237611731253\n",
      "58 (475136, 483328) 2695978 0.820722572661943\n",
      "59 (483328, 491520) 2695978 0.8176839721985862\n",
      "60 (491520, 499712) 2695978 0.8146453717352293\n",
      "61 (499712, 507904) 2695978 0.8116067712718724\n",
      "62 (507904, 516096) 2695978 0.8085681708085155\n",
      "63 (516096, 524288) 2695978 0.8055295703451586\n",
      "64 (524288, 532480) 2695978 0.8024909698818017\n",
      "65 (532480, 540672) 2695978 0.7994523694184448\n",
      "66 (540672, 548864) 2695978 0.7964137689550879\n",
      "67 (548864, 557056) 2695978 0.7933751684917311\n",
      "68 (557056, 565248) 2695978 0.7903365680283742\n",
      "69 (565248, 573440) 2695978 0.7872979675650172\n",
      "70 (573440, 581632) 2695978 0.7842593671016603\n",
      "71 (581632, 589824) 2695978 0.7812207666383034\n",
      "72 (589824, 598016) 2695978 0.7781821661749465\n",
      "73 (598016, 606208) 2695978 0.7751435657115896\n",
      "74 (606208, 614400) 2695978 0.7721049652482327\n",
      "75 (614400, 622592) 2695978 0.7690663647848759\n",
      "76 (622592, 630784) 2695978 0.766027764321519\n",
      "77 (630784, 638976) 2695978 0.7629891638581621\n",
      "78 (638976, 647168) 2695978 0.7599505633948052\n",
      "79 (647168, 655360) 2695978 0.7569119629314482\n",
      "80 (655360, 663552) 2695978 0.7538733624680913\n",
      "81 (663552, 671744) 2695978 0.7508347620047344\n",
      "82 (671744, 679936) 2695978 0.7477961615413775\n",
      "83 (679936, 688128) 2695978 0.7447575610780207\n",
      "84 (688128, 696320) 2695978 0.7417189606146638\n",
      "85 (696320, 704512) 2695978 0.7386803601513069\n",
      "86 (704512, 712704) 2695978 0.73564175968795\n",
      "87 (712704, 720896) 2695978 0.7326031592245931\n",
      "88 (720896, 729088) 2695978 0.7295645587612362\n",
      "89 (729088, 737280) 2695978 0.7265259582978792\n",
      "90 (737280, 745472) 2695978 0.7234873578345223\n",
      "91 (745472, 753664) 2695978 0.7204487573711655\n",
      "92 (753664, 761856) 2695978 0.7174101569078086\n",
      "93 (761856, 770048) 2695978 0.7143715564444517\n",
      "94 (770048, 778240) 2695978 0.7113329559810948\n",
      "95 (778240, 786432) 2695978 0.7082943555177379\n",
      "96 (786432, 794624) 2695978 0.705255755054381\n",
      "97 (794624, 802816) 2695978 0.7022171545910241\n",
      "98 (802816, 811008) 2695978 0.6991785541276672\n",
      "99 (811008, 819200) 2695978 0.6961399536643104\n",
      "100 (819200, 827392) 2695978 0.6931013532009535\n",
      "101 (827392, 835584) 2695978 0.6900627527375965\n",
      "102 (835584, 843776) 2695978 0.6870241522742396\n",
      "103 (843776, 851968) 2695978 0.6839855518108827\n",
      "104 (851968, 860160) 2695978 0.6809469513475258\n",
      "105 (860160, 868352) 2695978 0.6779083508841689\n",
      "106 (868352, 876544) 2695978 0.674869750420812\n",
      "107 (876544, 884736) 2695978 0.6718311499574552\n",
      "108 (884736, 892928) 2695978 0.6687925494940983\n",
      "109 (892928, 901120) 2695978 0.6657539490307414\n",
      "110 (901120, 909312) 2695978 0.6627153485673845\n",
      "111 (909312, 917504) 2695978 0.6596767481040275\n",
      "112 (917504, 925696) 2695978 0.6566381476406706\n",
      "113 (925696, 933888) 2695978 0.6535995471773137\n",
      "114 (933888, 942080) 2695978 0.6505609467139568\n",
      "115 (942080, 950272) 2695978 0.6475223462506\n",
      "116 (950272, 958464) 2695978 0.6444837457872431\n",
      "117 (958464, 966656) 2695978 0.6414451453238862\n",
      "118 (966656, 974848) 2695978 0.6384065448605293\n",
      "119 (974848, 983040) 2695978 0.6353679443971724\n",
      "120 (983040, 991232) 2695978 0.6323293439338155\n",
      "121 (991232, 999424) 2695978 0.6292907434704585\n",
      "122 (999424, 1007616) 2695978 0.6262521430071016\n",
      "123 (1007616, 1015808) 2695978 0.6232135425437448\n",
      "124 (1015808, 1024000) 2695978 0.6201749420803879\n",
      "125 (1024000, 1032192) 2695978 0.617136341617031\n",
      "126 (1032192, 1040384) 2695978 0.6140977411536741\n",
      "127 (1040384, 1048576) 2695978 0.6110591406903172\n",
      "128 (1048576, 1056768) 2695978 0.6080205402269603\n",
      "129 (1056768, 1064960) 2695978 0.6049819397636034\n",
      "130 (1064960, 1073152) 2695978 0.6019433393002465\n",
      "131 (1073152, 1081344) 2695978 0.5989047388368897\n",
      "132 (1081344, 1089536) 2695978 0.5958661383735327\n",
      "133 (1089536, 1097728) 2695978 0.5928275379101758\n",
      "134 (1097728, 1105920) 2695978 0.5897889374468189\n",
      "135 (1105920, 1114112) 2695978 0.586750336983462\n",
      "136 (1114112, 1122304) 2695978 0.5837117365201051\n",
      "137 (1122304, 1130496) 2695978 0.5806731360567482\n",
      "138 (1130496, 1138688) 2695978 0.5776345355933914\n",
      "139 (1138688, 1146880) 2695978 0.5745959351300345\n",
      "140 (1146880, 1155072) 2695978 0.5715573346666776\n",
      "141 (1155072, 1163264) 2695978 0.5685187342033207\n",
      "142 (1163264, 1171456) 2695978 0.5654801337399638\n",
      "143 (1171456, 1179648) 2695978 0.5624415332766068\n",
      "144 (1179648, 1187840) 2695978 0.5594029328132499\n",
      "145 (1187840, 1196032) 2695978 0.556364332349893\n",
      "146 (1196032, 1204224) 2695978 0.5533257318865362\n",
      "147 (1204224, 1212416) 2695978 0.5502871314231793\n",
      "148 (1212416, 1220608) 2695978 0.5472485309598224\n",
      "149 (1220608, 1228800) 2695978 0.5442099304964655\n",
      "150 (1228800, 1236992) 2695978 0.5411713300331086\n",
      "151 (1236992, 1245184) 2695978 0.5381327295697517\n",
      "152 (1245184, 1253376) 2695978 0.5350941291063948\n",
      "153 (1253376, 1261568) 2695978 0.5320555286430378\n",
      "154 (1261568, 1269760) 2695978 0.529016928179681\n",
      "155 (1269760, 1277952) 2695978 0.5259783277163241\n",
      "156 (1277952, 1286144) 2695978 0.5229397272529672\n",
      "157 (1286144, 1294336) 2695978 0.5199011267896103\n",
      "158 (1294336, 1302528) 2695978 0.5168625263262534\n",
      "159 (1302528, 1310720) 2695978 0.5138239258628965\n",
      "160 (1310720, 1318912) 2695978 0.5107853253995396\n",
      "161 (1318912, 1327104) 2695978 0.5077467249361827\n",
      "162 (1327104, 1335296) 2695978 0.5047081244728259\n",
      "163 (1335296, 1343488) 2695978 0.501669524009469\n",
      "164 (1343488, 1351680) 2695978 0.49863092354611205\n",
      "165 (1351680, 1359872) 2695978 0.49559232308275514\n",
      "166 (1359872, 1368064) 2695978 0.4925537226193982\n",
      "167 (1368064, 1376256) 2695978 0.4895151221560413\n",
      "168 (1376256, 1384448) 2695978 0.48647652169268446\n",
      "169 (1384448, 1392640) 2695978 0.48343792122932755\n",
      "170 (1392640, 1400832) 2695978 0.48039932076597064\n",
      "171 (1400832, 1409024) 2695978 0.47736072030261373\n",
      "172 (1409024, 1417216) 2695978 0.4743221198392569\n",
      "173 (1417216, 1425408) 2695978 0.47128351937589996\n",
      "174 (1425408, 1433600) 2695978 0.46824491891254305\n",
      "175 (1433600, 1441792) 2695978 0.46520631844918614\n",
      "176 (1441792, 1449984) 2695978 0.4621677179858293\n",
      "177 (1449984, 1458176) 2695978 0.4591291175224724\n",
      "178 (1458176, 1466368) 2695978 0.45609051705911546\n",
      "179 (1466368, 1474560) 2695978 0.45305191659575855\n",
      "180 (1474560, 1482752) 2695978 0.4500133161324017\n",
      "181 (1482752, 1490944) 2695978 0.4469747156690448\n",
      "182 (1490944, 1499136) 2695978 0.4439361152056879\n",
      "183 (1499136, 1507328) 2695978 0.44089751474233096\n",
      "184 (1507328, 1515520) 2695978 0.4378589142789741\n",
      "185 (1515520, 1523712) 2695978 0.4348203138156172\n",
      "186 (1523712, 1531904) 2695978 0.4317817133522603\n",
      "187 (1531904, 1540096) 2695978 0.4287431128889034\n",
      "188 (1540096, 1548288) 2695978 0.4257045124255465\n",
      "189 (1548288, 1556480) 2695978 0.4226659119621896\n",
      "190 (1556480, 1564672) 2695978 0.4196273114988327\n",
      "191 (1564672, 1572864) 2695978 0.4165887110354758\n",
      "192 (1572864, 1581056) 2695978 0.41355011057211893\n",
      "193 (1581056, 1589248) 2695978 0.410511510108762\n",
      "194 (1589248, 1597440) 2695978 0.4074729096454051\n",
      "195 (1597440, 1605632) 2695978 0.4044343091820482\n",
      "196 (1605632, 1613824) 2695978 0.40139570871869135\n",
      "197 (1613824, 1622016) 2695978 0.39835710825533444\n",
      "198 (1622016, 1630208) 2695978 0.3953185077919775\n",
      "199 (1630208, 1638400) 2695978 0.3922799073286206\n",
      "200 (1638400, 1646592) 2695978 0.38924130686526376\n",
      "201 (1646592, 1654784) 2695978 0.38620270640190685\n",
      "202 (1654784, 1662976) 2695978 0.38316410593854994\n",
      "203 (1662976, 1671168) 2695978 0.380125505475193\n",
      "204 (1671168, 1679360) 2695978 0.37708690501183617\n",
      "205 (1679360, 1687552) 2695978 0.37404830454847926\n",
      "206 (1687552, 1695744) 2695978 0.37100970408512235\n",
      "207 (1695744, 1703936) 2695978 0.36797110362176544\n",
      "208 (1703936, 1712128) 2695978 0.3649325031584086\n",
      "209 (1712128, 1720320) 2695978 0.3618939026950517\n",
      "210 (1720320, 1728512) 2695978 0.35885530223169476\n",
      "211 (1728512, 1736704) 2695978 0.35581670176833785\n",
      "212 (1736704, 1744896) 2695978 0.352778101304981\n",
      "213 (1744896, 1753088) 2695978 0.3497395008416241\n",
      "214 (1753088, 1761280) 2695978 0.3467009003782672\n",
      "215 (1761280, 1769472) 2695978 0.34366229991491026\n",
      "216 (1769472, 1777664) 2695978 0.3406236994515534\n",
      "217 (1777664, 1785856) 2695978 0.3375850989881965\n",
      "218 (1785856, 1794048) 2695978 0.3345464985248396\n",
      "219 (1794048, 1802240) 2695978 0.3315078980614827\n",
      "220 (1802240, 1810432) 2695978 0.3284692975981258\n",
      "221 (1810432, 1818624) 2695978 0.3254306971347689\n",
      "222 (1818624, 1826816) 2695978 0.322392096671412\n",
      "223 (1826816, 1835008) 2695978 0.3193534962080551\n",
      "224 (1835008, 1843200) 2695978 0.31631489574469823\n",
      "225 (1843200, 1851392) 2695978 0.3132762952813413\n",
      "226 (1851392, 1859584) 2695978 0.3102376948179844\n",
      "227 (1859584, 1867776) 2695978 0.3071990943546275\n",
      "228 (1867776, 1875968) 2695978 0.30416049389127064\n",
      "229 (1875968, 1884160) 2695978 0.30112189342791373\n",
      "230 (1884160, 1892352) 2695978 0.2980832929645568\n",
      "231 (1892352, 1900544) 2695978 0.2950446925011999\n",
      "232 (1900544, 1908736) 2695978 0.29200609203784306\n",
      "233 (1908736, 1916928) 2695978 0.28896749157448615\n",
      "234 (1916928, 1925120) 2695978 0.28592889111112924\n",
      "235 (1925120, 1933312) 2695978 0.2828902906477723\n",
      "236 (1933312, 1941504) 2695978 0.27985169018441547\n",
      "237 (1941504, 1949696) 2695978 0.27681308972105856\n",
      "238 (1949696, 1957888) 2695978 0.27377448925770165\n",
      "239 (1957888, 1966080) 2695978 0.27073588879434474\n",
      "240 (1966080, 1974272) 2695978 0.2676972883309879\n",
      "241 (1974272, 1982464) 2695978 0.26465868786763097\n",
      "242 (1982464, 1990656) 2695978 0.26162008740427406\n",
      "243 (1990656, 1998848) 2695978 0.25858148694091715\n",
      "244 (1998848, 2007040) 2695978 0.2555428864775603\n",
      "245 (2007040, 2015232) 2695978 0.2525042860142034\n",
      "246 (2015232, 2023424) 2695978 0.24946568555084647\n",
      "247 (2023424, 2031616) 2695978 0.2464270850874896\n",
      "248 (2031616, 2039808) 2695978 0.24338848462413268\n",
      "249 (2039808, 2048000) 2695978 0.2403498841607758\n",
      "250 (2048000, 2056192) 2695978 0.23731128369741888\n",
      "251 (2056192, 2064384) 2695978 0.234272683234062\n",
      "252 (2064384, 2072576) 2695978 0.2312340827707051\n",
      "253 (2072576, 2080768) 2695978 0.2281954823073482\n",
      "254 (2080768, 2088960) 2695978 0.2251568818439913\n",
      "255 (2088960, 2097152) 2695978 0.2221182813806344\n",
      "256 (2097152, 2105344) 2695978 0.2190796809172775\n",
      "257 (2105344, 2113536) 2695978 0.21604108045392062\n",
      "258 (2113536, 2121728) 2695978 0.2130024799905637\n",
      "259 (2121728, 2129920) 2695978 0.20996387952720683\n",
      "260 (2129920, 2138112) 2695978 0.20692527906384992\n",
      "261 (2138112, 2146304) 2695978 0.20388667860049303\n",
      "262 (2146304, 2154496) 2695978 0.20084807813713612\n",
      "263 (2154496, 2162688) 2695978 0.19780947767377924\n",
      "264 (2162688, 2170880) 2695978 0.19477087721042233\n",
      "265 (2170880, 2179072) 2695978 0.19173227674706544\n",
      "266 (2179072, 2187264) 2695978 0.18869367628370853\n",
      "267 (2187264, 2195456) 2695978 0.18565507582035165\n",
      "268 (2195456, 2203648) 2695978 0.18261647535699474\n",
      "269 (2203648, 2211840) 2695978 0.17957787489363786\n",
      "270 (2211840, 2220032) 2695978 0.17653927443028095\n",
      "271 (2220032, 2228224) 2695978 0.17350067396692406\n",
      "272 (2228224, 2236416) 2695978 0.17046207350356715\n",
      "273 (2236416, 2244608) 2695978 0.16742347304021027\n",
      "274 (2244608, 2252800) 2695978 0.16438487257685336\n",
      "275 (2252800, 2260992) 2695978 0.16134627211349648\n",
      "276 (2260992, 2269184) 2695978 0.1583076716501396\n",
      "277 (2269184, 2277376) 2695978 0.15526907118678268\n",
      "278 (2277376, 2285568) 2695978 0.1522304707234258\n",
      "279 (2285568, 2293760) 2695978 0.1491918702600689\n",
      "280 (2293760, 2301952) 2695978 0.146153269796712\n",
      "281 (2301952, 2310144) 2695978 0.1431146693333551\n",
      "282 (2310144, 2318336) 2695978 0.1400760688699982\n",
      "283 (2318336, 2326528) 2695978 0.1370374684066413\n",
      "284 (2326528, 2334720) 2695978 0.13399886794328442\n",
      "285 (2334720, 2342912) 2695978 0.1309602674799275\n",
      "286 (2342912, 2351104) 2695978 0.12792166701657062\n",
      "287 (2351104, 2359296) 2695978 0.12488306655321371\n",
      "288 (2359296, 2367488) 2695978 0.12184446608985681\n",
      "289 (2367488, 2375680) 2695978 0.11880586562649992\n",
      "290 (2375680, 2383872) 2695978 0.11576726516314302\n",
      "291 (2383872, 2392064) 2695978 0.11272866469978612\n",
      "292 (2392064, 2400256) 2695978 0.10969006423642923\n",
      "293 (2400256, 2408448) 2695978 0.10665146377307233\n",
      "294 (2408448, 2416640) 2695978 0.10361286330971543\n",
      "295 (2416640, 2424832) 2695978 0.10057426284635854\n",
      "296 (2424832, 2433024) 2695978 0.09753566238300164\n",
      "297 (2433024, 2441216) 2695978 0.09449706191964474\n",
      "298 (2441216, 2449408) 2695978 0.09145846145628785\n",
      "299 (2449408, 2457600) 2695978 0.08841986099293095\n",
      "300 (2457600, 2465792) 2695978 0.08538126052957405\n",
      "301 (2465792, 2473984) 2695978 0.08234266006621715\n",
      "302 (2473984, 2482176) 2695978 0.07930405960286026\n",
      "303 (2482176, 2490368) 2695978 0.07626545913950336\n",
      "304 (2490368, 2498560) 2695978 0.07322685867614646\n",
      "305 (2498560, 2506752) 2695978 0.07018825821278957\n",
      "306 (2506752, 2514944) 2695978 0.06714965774943267\n",
      "307 (2514944, 2523136) 2695978 0.06411105728607577\n",
      "308 (2523136, 2531328) 2695978 0.061072456822718876\n",
      "309 (2531328, 2539520) 2695978 0.058033856359361986\n",
      "310 (2539520, 2547712) 2695978 0.05499525589600509\n",
      "311 (2547712, 2555904) 2695978 0.05195665543264819\n",
      "312 (2555904, 2564096) 2695978 0.048918054969291296\n",
      "313 (2564096, 2572288) 2695978 0.0458794545059344\n",
      "314 (2572288, 2580480) 2695978 0.0428408540425775\n",
      "315 (2580480, 2588672) 2695978 0.039802253579220605\n",
      "316 (2588672, 2596864) 2695978 0.03676365311586371\n",
      "317 (2596864, 2605056) 2695978 0.03372505265250681\n",
      "318 (2605056, 2613248) 2695978 0.03068645218914991\n",
      "319 (2613248, 2621440) 2695978 0.027647851725793014\n",
      "320 (2621440, 2629632) 2695978 0.024609251262436117\n",
      "321 (2629632, 2637824) 2695978 0.02157065079907922\n",
      "322 (2637824, 2646016) 2695978 0.018532050335722323\n",
      "323 (2646016, 2654208) 2695978 0.015493449872365428\n",
      "324 (2654208, 2662400) 2695978 0.01245484940900853\n",
      "325 (2662400, 2670592) 2695978 0.009416248945651634\n",
      "326 (2670592, 2678784) 2695978 0.006377648482294737\n",
      "327 (2678784, 2686976) 2695978 0.00333904801893784\n",
      "328 (2686976, 2695168) 2695978 0.0003004475555809432\n",
      "329 (2695168, 2695978) 2695978 0.0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "inputs = [K.learning_phase()]\n",
    "outputs = [model.get_layer(\"Dropout\").output]\n",
    "\n",
    "for c in columns:\n",
    "    inputs_c = model.get_layer(\"Input_\"+c).input\n",
    "    inputs.append(inputs_c)\n",
    "\n",
    "func = K.function(inputs, outputs)\n",
    "\n",
    "sample_size = data.shape[0]\n",
    "data_ae = []\n",
    "batches = make_batches(sample_size, 1024*8)\n",
    "\n",
    "myfile = open(path+'python_embeddings.csv', 'w', newline='')\n",
    "wrtr = csv.writer(myfile, delimiter=',', quotechar='\"')\n",
    "\n",
    "for batch_index, (batch_start, batch_end) in enumerate(batches):\n",
    "    print(batch_index, (batch_start, batch_end), sample_size, (sample_size - batch_end)/sample_size)\n",
    "    X_0 = [0]\n",
    "    for c in columns:\n",
    "        X_0.append(np.reshape(data[batch_start:batch_end][c], (-1, 1)))\n",
    "    yy = func(X_0)[0]\n",
    "    for row in yy:\n",
    "        wrtr.writerow(row)\n",
    "    myfile.flush()\n",
    "    \n",
    "myfile.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.to_csv(path+'python_activity_ids.csv', columns=[\"activity_id\"])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:datalab]",
   "language": "python",
   "name": "conda-env-datalab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
